# 缓存重要性

Redis是一个开源高性能的Key-Value存储系统,因为其极高的读写性能,丰富的数据类型,原子性的操作以及其他特性而被广发运用，Redis的应用场景包括且不限于以下场景:

- 用来做分布式缓存
- 用来做分布式锁
- 用来处理某些特定高并发业务,比如秒杀等。

虽然Redis本身具备了非常高的可用性,但是在实际应用中也会随着系统业务的复杂性，以及不合理的使用产生很多的问题，本文将讲述如何通过混沌工程来暴露可能存在的使用风险,提升缓存问题的应急能力。

# 架构假设

在实施混沌工程之前,我们先需要了解到业务是如何使用Redis的，由于Redis最常用就是作为分布式缓存,我们这里就拿简单的商品查询场景来举例,应用的基本信息如下:

- 业务场景是查询商品信息,首先查询缓存,如果没有查询到，则查询数据库.
- 使用Jedis连接Redis，并且使用了Jedis-pool的技术.
- Redis是自建的集群(当然也可以使用云服务),并且使用[Sentinel](https://redis.io/topics/sentinel)技术来提升集群的高可用性。





![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618226758656-2408a9b8-243b-4f4c-bd43-7ff126557638.png)

从Jedis配置,缓存查询,网络传输,服务端处理这条链路上,每个环节都有可能出现问题,借助混沌工程我们希望了解到问题发生时候对系统对业务的影响面是否符合我们的预期。

# 演练场景梳理

对于这么一个应用,我们该如何梳理演练场景呢？从客户端？服务端？网络？我们在实际演练过程中往往会陷入场景看上去很多，但是不知道如何入手的情况,要解决这个问题，可以按照这样的思路来做:

- 明确缓存监控的指标
- 分析影响这些指标可能的因素,故障场景,参数等
- 从客户端层面去制造故障,因为客户端层面影响面可控
- 从服务端层面去制造故障,更加真实,并且对于问题定位和排查的要求更高.
- 注入故障,观察指标的变化。

## 缓存监控指标

对于缓存,我们有几个可以监控的指标:

- 缓存QPS

QPS是最通用也是最易观察的指标。

- 缓存命中率

缓存未命中可能会在大流量下引发,穿透,击穿,雪崩等问题,如果业务没有做好应急处理,很容易把DB压垮。

- - 穿透:key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。
  - 击穿:key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。
  - 雪崩:当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。

- 缓存RT

缓存RT对业务的影响分成多个方面,如果RT变化较少,对于业务访问缓存很少次数的情况下影响可控,但是如果一条请求需要多次访问缓存,那么哪怕RT只是几毫秒的增长,也会因为访问次数过多引起总的RT增长过多,引发蝴蝶效应,造成业务异常.

- 缓存成功率

缓存的请求成功率过低也会造成和命中率一样的后果。

- 业务成功率

在对业务成功率要求较高的业务当中,缓存必定是作为一个弱依赖存在,当缓存出现问题,系统应该有其他兜底策略,但是随着系统越来越复杂,改造的越来越频繁，原本预计的缓存弱依赖也**会不经意间**被改造成强依赖,一旦出现这种情况,就会导致业务受损。

## 影响因素

由于影响系统的因素有很多,从机房,电源,集群服务,操作系统,应用配置等多因素,这里主要梳理一下从操作系统层面到应用层面的影响因素,

- 系统层面的影响因素有网络,磁盘,IO,内存,CPU等几个因素.
- 应用层面的影响有超时配置,连接池配置,查询不合理等因素.

我们结合缓存监控指标,影响因素,从客户端和服务端两个角度来分析(**这里的原因假设业务请求QPS保持稳定**)

###  客户端



| 因素                                | 模拟手段                      | 可能后果                                | 可能影响指标 |
| ----------------------------------- | ----------------------------- | --------------------------------------- | ------------ |
| 网络延迟                            | 6379端口网络延迟              | 读写请求RT变长连接池满                  | QPSRT成功率  |
| 网络断掉                            | 6379端口网络丢包              | 读写失败无法连接连接                    | QPSRT成功率  |
| 单次查询耗时过长                    | 如果Key过多,可以模拟Keys*查询 | 单次请求RT变长连接池占满                | QPSRT成功率  |
| 连接池设置不合理,连接池过小或者过高 | Jedis连接池占满               | 无法建立连接                            | QPSRT成功率  |
| 缓存未命中                          | Jedis返回值拦截               | 穿透击穿雪崩                            | 命中率       |
| 缓存未命中或者异常                  | Jedis返回值拦截Jedis抛异常    | 缓存强依赖,业务失败缓存未命中一样的后果 | 成功率       |

### 服务端



| 因素             | 模拟手段                      | 可能后果                                            | 可能影响指标 | 如何改进                                                     |
| ---------------- | ----------------------------- | --------------------------------------------------- | ------------ | ------------------------------------------------------------ |
| 磁盘空间不够     | 磁盘填充                      | 开启了AOF会导致日志无法写入无法备份缓存数据         | QPSRT成功率  | 监控磁盘利用率禁AOF                                          |
| 网络异常         | 端口延迟端口丢包              | 指定客户端请求超时                                  | QPSRT成功率  | 网络监控集群                                                 |
| 连接池满         | 建立网络连接                  | 无法分配新连接,客户端建连失败                       |              | 设置timeout和tcp-keeplive参数网络监控                        |
| 单次查询耗时过长 | 如果Key过多,可以模拟Keys*查询 | 单次请求RT变长连接池占满                            | QPSRT成功率  | 避免Keys*类查询RT监控                                        |
| IO读写过高       | 磁盘读写IO过高                | 读写变慢,响应超时                                   | QPSRT成功率  | 热点key监控主从持久化策略修改                                |
| 内存不够         | Jedis返回值拦截               | 内存不足导致AOF无法备份内存不足导致无法写入         | 命中率成功率 | 内存监控设定内存阈值,合理内存策略缓存设定失效,避免冷数据库过多 |
| CPU过高          | CPU满载                       | CPU会影响时间片的获取读写RT变长主从心跳服务出现异常 | 成功率RT     | 系统监控                                                     |

# 

# 演练实战

Talk is cheap,show me the experiment

脱离实践来谈理论都是空的,下面讲通过如何借助CHAOS故障演练平台来看下从客户端层面来评测业务对Redis的合理使用

## 演练系统

我们的业务场景很简单就是简单的商品购物车查询,为了便于理解,我们对系统做了逻辑简化,同时为了尽可能的模拟真实情况,我们制定了相关的业务指标:

- 整个系统分为首页和购物车页面
- 首页通过Dubbo来调用购物车接口，购物车服务端的接口超时设置为3000毫秒.
- 每一次购物车的内部查询,都需要查询50次的缓存(为了更好观看演练效果,次数稍微放大),每次缓存的操作在10毫秒左右.
- 购物车的内部查询优先走缓存,失败了以后再走数据库.
- 连接缓存的SDK使用java的JedisClient,设置的超时时间为100毫秒

核心查询代码如下:

```java
    //弱依赖缓存
    @Override
    public List<CartItem> viewCart(String userId) {
        try {
            for (int i = 0; i < 50; i++) {
                logger.info("query redis,count:" + i);
                redisRepository.getUserCartItems(userId);
            }
            return redisRepository.getUserCartItems(userId);
        } catch (Exception exception) {
            logger.error("get data from redis failed ,use local data", exception);
        }
        logger.info("get data from redis failed ,query from db");
        return cartDBRepository.findByUserId(userId).stream().map(this::fromCart).collect(Collectors.toList());
    }
```

相关的架构图如下:

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618394262037-4457e026-678a-4a25-a346-4d7128d0aa19.png)

## 演练假设

从**影响因素**里面可以看到影响Redis使用稳定性有很多原因,在这里我们挑选一个场景,**评测网络延迟对Redis使用的影响,来观察RT变化之后业务能否继续保持正常服务**

**基于网络延迟这个场景,我们可以提出这样的假设:**

- 缓存的RT变化不应该影响到购物车查询的成功率
- 由于缓存RT的增加,购物车查询RT会先增加,接着缓存RT增加到一定的值,使得缓存彻底无法访问,这个时候会触发缓存降级,而去走数据库,查询RT会回落。
- 虽然RT变化了,但是因为我们做了强弱依赖,购物车查询成功率不会有很明显的变化

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618451589328-0e347a73-ad04-4f2d-bfe4-164bbeb80338.png)

## 演练场景

针对上面的假设结合系统特征,我们可以设计出这几个演练场景:

- Redis延迟增加20MS,这个时候缓存累计耗时:50*20=1000MS，这个时候CartServiceRT小于配置的接口超时3000MS,业务正常.
- Redis延迟增加80MS,这个时候缓存累计耗时:50*80=4000MS，购物车内部查询继续,但是这个时候CartServiceRT大于配置的接口超时3000MS,这个时候购物车查询失败.
- Redis延迟增加10000MS,这个时候这个时候缓存超出了Jedis的超时配置时间100MS,使得缓存查询快速failover,查询走DB，业务正常。

## 演练实施

通过阿里云Chaos演练平台可以快速的配置以上的场景,并且结合平台提供的业务探活功能,可以快速实现整个故障演练的自动化评测，具体操作如下:

- 首先我们通过探针管理向Cart服务所在机器的安装演练探针

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618454344887-93664589-45a0-418d-9a9a-1fbcfddec676.png)

- 安装完毕之后,我们就可以开始创建演练场景,这里面我们使用了**网络延迟**这一故障能力,点击**新建演练**,选择机器之后，挑选演练场景**主机内网络延迟**
- 选择场景之后,配置缓存端口6379,以及延迟时间.
- 由于我们要观测演练前和故障注入后系统的业务情况，因此除了故障注入节点之外,我们还需要增加业务探活的节点.

- - chaos提供了类似K8S的探活功能,可以通过访问指定接口来判断业务是否可用,考虑到网络细微波动以及其他因素影响,并且支持多种参数配置(熟悉K8S的同学不会陌生).

- - - failureThreshold-重试次数，重试几次失败后判断为校验失败，这个例子我们设置为5次
    - periodSeconds-探测时间间隔,这个例子我们配置成2秒
    - successThreshold-连续成功几次算成功,这个例子里面我们配置2次.
    - url-需要探测的url, 这个例子里面我们配置成购物车的查询地址.
    - method-get or post方法，这个例子里面我们设置为get

最终我们配置成如下完整演练流程:

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618454854588-8616b5d5-bcb7-470f-a4fe-44b98d4444eb.png)

**演练重要的一点就是在演练前需要确保业务系统处于正常状态,所以在故障注入前我们需要判断下是否可用.**

配置完毕之后,我们就可以发起自动演练,自动探测，最终得出结论(Chaos支持演练节点自动推进,也支持手动一步步推进)

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618455121654-dcf630bd-25b8-431f-ab19-962b201954b9.png)

最终的运行结果如我们的假设一样,当延迟注入80MS之后,购物车不可用了,那么当注入20MS和10000MS，虽然购物车可用，但是是否真如预期那样，一个是RT延长但是接口未超时，一个是缓存降级导致的业务成功呢？

我们点击校验购物车是否可用的节点来查看.

- 首先看演练一开始的探活节点,点击校验购物车是否可用,但是查看探活记录.

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618455327477-94ceb698-ad41-44bd-9bd0-35cb69c71f84.png)

查询RT处于正常范围内

- 接着我们看注入20MS之后的探活节点

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618455379616-04cd4d86-db40-4a42-bb27-c5025f8cf6f9.png)

发现业务RT明显增长，但是还是在超时的3秒内，因此业务正常.

- 接着我们看注入80MS之后的探活节点,发现业务异常

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618455710605-673bd252-e5b0-4b2f-adc5-a26caec97522.png)

- 最后我们看注入10000MS之后的探活节点,发现RT回落,业务正常,因为缓存failover导致，走了DB导致RT相对延长。

![img](https://intranetproxy.alipay.com/skylark/lark/0/2021/png/4690/1618455733787-8cc3cf7a-5cb6-4e93-8842-9ed0ba60dd23.png)

通过上面的演练我们证明了以下几点:

- 缓存RT轻微增长,对业务影响可控,但是如果业务内部存在多次的缓存查询,会导致整体RT增加明显，就像这个例子里面的,RT延长处于客户端连接超时范围内,无法触发弱依赖降低,但是整个接口RT超时,最终导致业务受损.
- 在缓存RT增长很明显的情况下,缓存降级策略能够正常生效,使得业务正常访问，当然在实际情况中,这种兜底策略可能导致DB直接被打爆。

## 演练价值

通过对不同网络延迟的演练，我们可以了解到缓存RT变化对系统造成的影响,以及我们的防护策略有效性,随着业务规模的不断增长,这个简单的业务系统也会面临新的问题:

- 在某次重构中,又新增加了缓存查询，结果导致20MS的延迟就可能导致接口整体超时,这种问题该如何预防?
- 业务逻辑简单时候,我们能够很好的分析强弱依赖,但是随着微服务的膨胀,以及代码多次重构，可能原有的弱依赖在某次变更中变成了强依赖,这种通过功能测试是无法发现的,这种问题该如何预防?
- 我们这里Jedis设置的超时时间是100MS,不同业务对RT的要求不一样,这里面的100MS是否符合所有业务诉求,对于RT要求高的业务该如何正确的设置超时时间？

上述的一些问题都要通过故障演练来进行发现,在日常的发布,架构升级中除了功能测试,性能测试的回归，还需要进行常态化的故障演练,同时演练的形态和场景复杂性也要不断扩充,对于故障演练来说,**难的不是注入手段,而是对业务架构,业务场景的理解，故障注入不是目的,演练的目的是加深对系统的理解,这样当真实的问题来临时候,才能更加有信心的去处理.**



附录

- https://tech.meituan.com/2016/12/02/performance-tunning.html
- https://tech.meituan.com/2017/03/17/cache-about.html
- Asynchronous AOF fsync is taking too long
- https://www.cnblogs.com/zengkefu/p/5634746.html
-  [redis 持久化 AOF和 RDB 引起的生产故障](https://www.cnblogs.com/yangxiaoyi/p/7806406.html)
- https://zhuanlan.zhihu.com/p/134419203
- [今天要解决的问题主要有两部分：**Redis的快照持久化ERROR，还有服务器磁盘不够的异常。**](https://blog.csdn.net/mygodit/article/details/108995706)
- https://www.cnblogs.com/bigjunoba/p/9136106.html
- https://help.aliyun.com/document_detail/98726.html?spm=a2c4g.11186623.6.889.242b349aroTO56#section-m2c-5kr-zfb